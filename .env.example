# API Keys
GOOGLE_MAPS_API_KEY=your_google_maps_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# GitHub Integration
# Optional repository-wide fallback token for unauthenticated routes.
# Prefer saving per-user/project tokens via /workflows/services/git/config.
# Never commit real tokens.
GITHUB_TOKEN=your_github_pat_here

# Diagnostics logging for Git Files Service (opt-in)
# Set to 1|true|yes|debug to enable request/response/config logs
LOG_GIT=1

# Development Settings
NODE_ENV=development
FLASK_ENV=development 

# Encryption for credentials storage
# Used by workflows/creds for AES-256-GCM encryption of stored secrets.
# Generate a strong key (recommended 32 random bytes, base64-encoded):
#   macOS/Linux: openssl rand -base64 32
#   Node.js: node -e "console.log(require('crypto').randomBytes(32).toString('base64'))"
ENCRYPTION_SECRET_KEY=your_base64_secret_here

# --- Jobs producer/consumer bridge ---
# Base URL for the Workflows service (where events/stream, callbacks, cursors live)
WORKFLOWS_BASE_URL=http://localhost:8080
# Audience for ID token when calling the workflows service (in Cloud Run, set to the service URL)
WORKFLOWS_AUDIENCE=http://localhost:8080

# Base URL for the SSE consumer service (private Cloud Run or local dev)
CONSUMER_BASE_URL=http://localhost:8090

# Optional dev-only bearer shared secret used by consumer and producer
SERVICE_AUTH_TOKEN=dev-shared-token

# Cloud Run Job trigger settings
# GCP project id and region for the Cloud Run Job
GCP_PROJECT=your-gcp-project-id
CLOUD_RUN_LOCATION=us-central1
# The Cloud Run Job name that runs the producer container
PRODUCER_CLOUD_RUN_JOB_NAME=producer-bridge
# Optional container name override (if job defines multiple containers)
PRODUCER_CONTAINER_NAME=producer

# Tuning knobs
EVENTS_HEARTBEAT_MS=15000
RECONNECT_BACKOFF_MS=1000
RUN_COMMAND_TIMEOUT_SECONDS=120
READ_FILE_MAX_BYTES=200000
OUTPUT_MAX_BYTES=50000

# --- GCS project mirror to consumer WORK_ROOT (JSON API based) ---
# When a downscoped token (X-Gcs-Token) and bucket/prefix are provided, the consumer
# can mirror objects to its local WORK_ROOT. Uploads back to GCS are enabled by default.

# Consumer-side environment
# Bucket that contains per-tenant workspaces to mirror
GCS_BUCKET=your-shared-bucket-name
# Prefix template within the bucket; variables: {userId},{projectId},{workspaceId},{sessionId}
GCS_PREFIX_TEMPLATE={userId}/{projectId}/{sessionId}/
# Enable initial sync when a session stream is established (default 1)
SYNC_ON_START=1
# Periodic sync interval in ms while a stream is open (default 15000)
SYNC_INTERVAL_MS=15000
# Optional: override GCS JSON API base and concurrency
GCS_API_BASE=https://www.googleapis.com
GCS_DOWNLOAD_CONCURRENCY=8
# Enable uploads from consumer back to GCS (default 1). Set to 0 to disable.
GCS_ENABLE_UPLOAD=1
# Parallel uploads
GCS_UPLOAD_CONCURRENCY=4
# Where the consumer stores mirrored files (inside container)
WORK_ROOT_BASE=/workspace

# Producer-side (local/dev only): optional pass-through downscoped token
# In production the producer will mint a per-tenant CAB token and set the X-Gcs-Token header.
GCS_TOKEN=

# GCS downscope config (producer)
# Default permission mode is explicit (no need to set this var):
#   availablePermissions: [storage.objects.get, storage.objects.list]
# Set to "role" to use: availablePermissions: [inRole:roles/storage.objectViewer]
# When uploads are enabled (default), the producer widens the CAB to include
# storage.objects.create so the consumer can upload with ifGenerationMatch.
#GCS_CAB_PERMISSION_MODE=explicit

# Diagnostics for GCS downscope and consumer list/upload requests
# Set to 1|true|yes to enable detailed logging in producer/consumer
#GCS_DEBUG=1
